{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b7d7490-f4f6-44fa-8487-a45a0b7ed94a",
   "metadata": {},
   "source": [
    "# Question 5 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2707769-26c8-404d-b9e3-4e2a1c0ef2f0",
   "metadata": {},
   "source": [
    "## Setting Up Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8075bd0-e8ef-47d5-98b3-b588ee123cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import logging\n",
    "import functools\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession #library to write data to the database asynchronously\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import text\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Setup logging format\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Create asynchronous connection to 'master' database\n",
    "engine = create_async_engine(\n",
    "    \"mssql+aioodbc://sa:!Hartree123!@localhost:1433/master?\"\n",
    "    \"driver=ODBC+Driver+18+for+SQL+Server&TrustServerCertificate=yes\",\n",
    "    echo=False\n",
    ")\n",
    "\n",
    "# Create an asynchronous session object for executing SQL commands\n",
    "AsyncSessionLocal = sessionmaker(engine, expire_on_commit=False, class_=AsyncSession)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd210430-b926-469a-ad6d-b38cb47452e5",
   "metadata": {},
   "source": [
    "## Asynchronous Database Insert with Logging & Data Reading\n",
    "\n",
    "I use a log_insert decorator to wrap asynchronous functions, specificllay the bulk insert function. Then the bulk insert function uses asynchronous methods to ensure that the operation does not block the event loop, so improving the performance when dealing with I/O-bound tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca4ecd0-2e65-4fc6-8368-ea58f98a72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async logging decorator: Log start and end of async function\n",
    "def log_insert(func):\n",
    "    @functools.wraps(func)\n",
    "    async def wrapper(*args, **kwargs):\n",
    "        logging.info(\"Starting SQL insert operation.\")\n",
    "        result = await func(*args, **kwargs)\n",
    "        logging.info(\"Completed SQL insert operation.\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "#Async bulk insert - insertin records into metal_prices table\n",
    "@log_insert\n",
    "async def async_bulk_insert(session, records):\n",
    "    insert_stmt = text(\"\"\"\n",
    "        INSERT INTO metal_prices (\n",
    "            date, metal, price,\n",
    "            macd_fast, macds_fast, macdh_fast,\n",
    "            macd_med, macds_med, macdh_med,\n",
    "            macd_slow, macds_slow, macdh_slow,\n",
    "            rsi\n",
    "        ) VALUES (\n",
    "            :date, :metal, :price,\n",
    "            :macd_fast, :macds_fast, :macdh_fast,\n",
    "            :macd_med, :macds_med, :macdh_med,\n",
    "            :macd_slow, :macds_slow, :macdh_slow,\n",
    "            :rsi\n",
    "        )\n",
    "    \"\"\")\n",
    "    await session.execute(insert_stmt, records)\n",
    "    await session.commit()\n",
    "\n",
    "# async data read - retriveing and printing record count\n",
    "async def async_read_data():\n",
    "    \"\"\"Reads from the database; for example, returns a count of records.\"\"\"\n",
    "    async with AsyncSessionLocal() as session:\n",
    "        result = await session.execute(text(\"SELECT COUNT(*) FROM metal_prices\"))\n",
    "        count = result.scalar()\n",
    "        print(f\"Record count: {count}\")\n",
    "        return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17894973-3667-4e46-a8e4-66669fd083f4",
   "metadata": {},
   "source": [
    "## Processing Metal Data with Technical Indicators\n",
    "\n",
    "I refactored the MACD calculation method from my solution in Question 3 to eliminate redundancy. Before I computed each set of MACD values manually, which was repetitive. Now by using a loop over a dictionary of MACD parameters and leveraging the ta library from pandas, the code is computationally more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b57a8f5-c0bd-482a-804f-d5528692b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metal(metal, df):\n",
    "    # Copy relevant columns and rename the metal column to 'price'\n",
    "    df_temp = df[['Dates', metal]].copy()\n",
    "    df_temp = df_temp.rename(columns={metal: 'price'})\n",
    "    \n",
    "    # Filter for years 2020 and 2021 and convert price to numeric\n",
    "    df_temp = df_temp[df_temp['Dates'].dt.year.isin([2020, 2021])]\n",
    "    df_temp['price'] = pd.to_numeric(df_temp['price'], errors='coerce')\n",
    "    \n",
    "    # Calculate rsi indicators\n",
    "    df_temp['rsi'] = ta.rsi(df_temp['price'], length=14)\n",
    "\n",
    "    # Defining MACD parameter sets for calculations\n",
    "    macd_params = {\n",
    "        'fast': {'fast': 12, 'slow': 26, 'signal': 9},\n",
    "        'med':  {'fast': 19, 'slow': 39, 'signal': 9},\n",
    "        'slow': {'fast': 26, 'slow': 52, 'signal': 9}\n",
    "    }\n",
    "    \n",
    "    # Looping over parameter sets to calculate MACD values\n",
    "    for key, params in macd_params.items():\n",
    "        macd_results = ta.macd(df_temp['price'], \n",
    "                               fast=params['fast'], \n",
    "                               slow=params['slow'], \n",
    "                               signal=params['signal'])\n",
    "        df_temp[f'macd_{key}']  = macd_results[f'MACD_{params[\"fast\"]}_{params[\"slow\"]}_{params[\"signal\"]}']\n",
    "        df_temp[f'macds_{key}'] = macd_results[f'MACDs_{params[\"fast\"]}_{params[\"slow\"]}_{params[\"signal\"]}']\n",
    "        df_temp[f'macdh_{key}'] = macd_results[f'MACDh_{params[\"fast\"]}_{params[\"slow\"]}_{params[\"signal\"]}']\n",
    "    \n",
    "    # Add metal identifier column\n",
    "    df_temp['metal'] = metal\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2f0fa6-0cb5-4f4d-838d-538a6aa0170b",
   "metadata": {},
   "source": [
    "## Async Data Loading, Processing, and Bulk DB Operations\n",
    "\n",
    "Main function where the processed data is converted to a list of dictionaries and inserted into the database asynchronously. Then the code concurently reads the database record count 5 times using asyncio.gather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96de2a74-9f62-44d8-823e-bfb618c8caa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macd_fast already exists\n",
      "macds_fast already exists\n",
      "macdh_fast already exists\n",
      "macd_med already exists\n",
      "macds_med already exists\n",
      "macdh_med already exists\n",
      "macd_slow already exists\n",
      "macds_slow already exists\n",
      "macdh_slow already exists\n",
      "rsi already exists\n",
      "Database and table setup complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 16:34:15,395 - INFO - Starting SQL insert operation.\n",
      "2025-03-22 16:34:15,878 - INFO - Completed SQL insert operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 2020 and 2021 Copper and Zinc records successfully inserted.\n",
      "Record count: 1046\n",
      "Record count: 1046\n",
      "Record count: 1046\n",
      "Record count: 1046\n",
      "Record count: 1046\n",
      "Concurrent read results: [1046, 1046, 1046, 1046, 1046]\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    # Load CSV and clean csv data\n",
    "    df = pd.read_csv('/Users/giacomofiorani/Desktop/Hartree/OilDesk-Intern-Assessment/data/MarketData.csv', header=None)\n",
    "    df = df.iloc[7:].reset_index(drop=True)\n",
    "    df.columns = ['Dates', 'COPPER', 'ALUMINIUM', 'ZINC', 'LEAD', 'TIN', 'FUTURE']\n",
    "    df['Dates'] = pd.to_datetime(df['Dates'], dayfirst=True)\n",
    "    \n",
    "    # Delete all existing records asynchronously to prevent duplication\n",
    "    async with engine.begin() as conn:\n",
    "        await conn.execute(text(\"DELETE FROM metal_prices\"))\n",
    "        await conn.commit()\n",
    "    \n",
    "    # Defining list of additional columns needed for technical indicators\n",
    "    additional_cols = [\n",
    "        'macd_fast', 'macds_fast', 'macdh_fast',\n",
    "        'macd_med', 'macds_med', 'macdh_med',\n",
    "        'macd_slow', 'macds_slow', 'macdh_slow',\n",
    "        'rsi'\n",
    "    ]\n",
    "\n",
    "    #asynchronous connect to database and ensure each column exists\n",
    "    async with engine.connect() as conn:\n",
    "        for col in additional_cols:\n",
    "            #ensure column exists in metal_prices table\n",
    "            result = await conn.execute(\n",
    "                text(\"\"\"\n",
    "                    SELECT COUNT(*) FROM INFORMATION_SCHEMA.COLUMNS\n",
    "                    WHERE TABLE_NAME = 'metal_prices' AND COLUMN_NAME = :col\n",
    "                \"\"\"), {\"col\": col}\n",
    "            )\n",
    "            count = result.scalar()\n",
    "            #if column missing add it\n",
    "            if count == 0:\n",
    "                await conn.execute(text(f\"ALTER TABLE metal_prices ADD {col} FLOAT\"))\n",
    "                print(f\"{col} successfully added\")\n",
    "            else:\n",
    "                print(f\"{col} already exists\")\n",
    "        #asynchronous commit to database\n",
    "        await conn.commit()\n",
    "    print(\"Database and table setup complete.\")\n",
    "    \n",
    "    # Process both metals concurrently using Joblib's Parallel with delayed\n",
    "    processed_dfs = Parallel(n_jobs=2)(delayed(process_metal)(metal, df) for metal in ['COPPER', 'ZINC'])\n",
    "    final_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    final_df = final_df.rename(columns={'Dates': 'date'})\n",
    "    final_df = final_df.where(final_df.notnull(), None)\n",
    "    \n",
    "    # Convert DataFrame to a list of dictionaries for bulk insert\n",
    "    records = final_df.to_dict(orient='records')\n",
    "    records = [{k: (None if pd.isna(v) else v) for k, v in rec.items()} for rec in records]\n",
    "    \n",
    "    # Asynchronously bulk insert records\n",
    "    async with AsyncSessionLocal() as session:\n",
    "        await async_bulk_insert(session, records)\n",
    "    print(\"All 2020 and 2021 Copper and Zinc records successfully inserted.\")\n",
    "    \n",
    "    # Concurrently read from the database 5 times using asyncio.gather()\n",
    "    read_tasks = [async_read_data() for _ in range(5)]\n",
    "    results = await asyncio.gather(*read_tasks)\n",
    "    print(\"Concurrent read results:\", results)\n",
    "\n",
    "# Entry point: Run the program by awaiting the main() async function when this script is executed.\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ed3c3-1385-4fcb-8935-6ee7297d1546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
